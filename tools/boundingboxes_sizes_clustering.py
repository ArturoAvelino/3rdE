import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Union, Optional, Tuple, Dict, Any
import logging
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler


class BoundingBoxClusteringProcessor:
    """
    A comprehensive clustering processor for analyzing bounding box dimensions from CSV files.

    ## Purpose and Overview

    The BoundingBoxClusteringProcessor class is designed to perform advanced clustering analysis
    on bounding box data extracted from computer vision annotation datasets. It processes CSV files
    containing bounding box dimensions and applies unsupervised machine learning clustering algorithms
    to identify patterns and groupings within the data. This class is particularly valuable for
    understanding the distribution of object sizes in image datasets and for preprocessing data
    for object detection and computer vision applications.

    ## Key Capabilities and Features

    ### Data Processing and Validation
    - **CSV Data Loading**: Reads and validates CSV files with bounding box dimensions
    - **Data Integrity Checking**: Validates required columns and removes invalid entries
    - **Dimension Redefinition**: Optional feature to normalize dimensions by making the larger
      value the width and smaller value the height for consistent analysis
    - **Data Preprocessing**: Handles missing values, filters invalid data, and prepares data for clustering

    ### Clustering Algorithm Support
    - **K-means Clustering**: Partitions data into K clusters using centroid-based approach
    - **DBSCAN Clustering**: Density-based clustering that can identify clusters of varying
      shapes and automatically detect outliers/noise points
    - **Data Standardization**: Applies StandardScaler to normalize features for better clustering performance
    - **Flexible Parameter Configuration**: Supports algorithm-specific parameters for fine-tuning

    ### Visualization and Analysis
    - **2D Scatter Plots**: Creates publication-quality visualizations showing cluster assignments
    - **Color-coded Clusters**: Each cluster is displayed in a different color for easy interpretation
    - **Centroid Visualization**: K-means cluster centers are highlighted with distinctive markers
    - **Noise Point Identification**: DBSCAN noise points are clearly marked for analysis
    - **Customizable Plot Settings**: Configurable figure size, DPI, and output formats

    ### Output Generation and Reporting
    - **Cluster Centers Analysis**: Generates detailed text reports with cluster center coordinates
    - **Statistical Summaries**: Provides comprehensive clustering statistics and metadata
    - **File Export Options**: Saves visualizations and reports to specified output directories
    - **Comprehensive Logging**: Detailed logging for debugging and process monitoring

    ## Expected Input Data Format

    The class expects CSV files with the following structure (typically generated by BatchBoundingBoxProcessor):

    ```
    file_name,id,box_width,box_height
    image001.jpg,0,150,120
    image001.jpg,1,200,180
    image002.jpg,0,175,140
    ...
    ```

    Required columns:
    - **file_name**: Name of the source image file
    - **id**: Unique identifier for each annotation
    - **box_width**: Width of the bounding box in pixels
    - **box_height**: Height of the bounding box in pixels

    ## Method Descriptions

    ### Core Processing Methods

    #### `__init__(cluster_number, input_file_path, output_path, output_filename)`
    Initializes the clustering processor with configuration parameters. Validates input files,
    creates output directories, and sets up logging infrastructure.

    #### `load_csv_data()`
    Loads CSV data from the specified input file and performs comprehensive validation:
    - Checks for required columns
    - Validates numeric data types
    - Removes invalid entries (negative values, missing data)
    - Provides detailed logging of data quality issues

    #### `redefine_dimensions(inplace=True)`
    Normalizes bounding box dimensions by ensuring the larger dimension becomes box_width
    and the smaller becomes box_height. This is useful for consistent analysis when
    orientation doesn't matter.

    #### `prepare_clustering_data()`
    Extracts and prepares the box_width and box_height columns for clustering analysis.
    Converts pandas DataFrame to numpy array format required by scikit-learn.

    ### Clustering Algorithm Methods

    #### `apply_kmeans_clustering(random_state=42)`
    Applies K-means clustering algorithm:
    - Standardizes data using StandardScaler
    - Creates K clusters using the specified cluster_number
    - Returns cluster labels and fitted model
    - Provides reproducible results through random_state parameter

    #### `apply_dbscan_clustering(eps=0.5, min_samples=5)`
    Applies DBSCAN (Density-Based Spatial Clustering) algorithm:
    - Automatically determines the number of clusters
    - Identifies noise points (outliers)
    - Handles clusters of arbitrary shapes
    - Parameters: eps (neighborhood radius), min_samples (minimum cluster size)

    ### Visualization and Output Methods

    #### `create_scatter_plot(save_plot=True, plot_filename=None, figure_size=(12,8), dpi=300)`
    Generates comprehensive 2D scatter plots:
    - Color-codes each cluster for visual distinction
    - Highlights cluster centers (K-means) or noise points (DBSCAN)
    - Includes detailed legends and axis labels
    - Supports high-resolution output for publications

    #### `save_cluster_centers(centers_filename=None)`
    Creates detailed text reports containing:
    - Algorithm-specific cluster center coordinates
    - Number of data points in each cluster
    - Processing metadata and timestamps
    - Statistical summaries of clustering results

    #### `get_clustering_summary()`
    Generates comprehensive clustering statistics:
    - Algorithm type and parameters
    - Number of clusters and data points
    - Cluster size distributions
    - Data range and basic statistics

    ### Workflow Management

    #### `process_complete_workflow(algorithm='kmeans', redefine_dims=False, **algorithm_params)`
    Executes the complete clustering pipeline:
    1. Loads and validates input data
    2. Optionally redefines dimensions
    3. Prepares data for clustering
    4. Applies specified clustering algorithm
    5. Generates visualizations and reports
    6. Returns comprehensive summary statistics

    ## Usage Examples

    ### Basic K-means Clustering
    ```python
    processor = BoundingBoxClusteringProcessor(
        cluster_number=5,
        input_file_path="bounding_boxes.csv",
        output_path="clustering_results",
        output_filename="kmeans_analysis"
    )

    summary = processor.process_complete_workflow(
        algorithm='kmeans',
        redefine_dims=True,
        random_state=42
    )
    ```

    ### DBSCAN Clustering with Custom Parameters
    ```python
    processor = BoundingBoxClusteringProcessor(
        cluster_number=5,  # Not used for DBSCAN but required
        input_file_path="bounding_boxes.csv",
        output_path="clustering_results",
        output_filename="dbscan_analysis"
    )

    summary = processor.process_complete_workflow(
        algorithm='dbscan',
        redefine_dims=True,
        eps=0.3,
        min_samples=10
    )
    ```

    ### Step-by-Step Processing
    ```python
    processor = BoundingBoxClusteringProcessor(
        cluster_number=3,
        input_file_path="data.csv",
        output_path="output",
        output_filename="analysis"
    )

    # Load and process data
    processor.load_csv_data()
    processor.redefine_dimensions()
    processor.prepare_clustering_data()

    # Apply clustering
    labels, model = processor.apply_kmeans_clustering()

    # Generate outputs
    processor.create_scatter_plot()
    processor.save_cluster_centers()

    # Get summary
    summary = processor.get_clustering_summary()
    ```

    ## Output Files Generated

      - **Raw Data Plot**: `{output_filename}_scatter_raw_data.png`
      - **Clustered Data Plot**: `{output_filename}_scatter_plot.png`
      - **Cluster Centers**: `{output_filename}_cluster_centers.txt`

    ### Visualization Files
    - **Scatter Plot**: `{output_filename}_scatter_plot.png`
      - High-resolution 2D scatter plot with cluster coloring
      - Includes cluster centers, legends, and axis labels
      - Suitable for publications and presentations

    ### Analysis Reports
    - **Cluster Centers**: `{output_filename}_cluster_centers.txt`
      - Detailed coordinates of cluster centers
      - Cluster size statistics
      - Processing metadata and timestamps

    ## Algorithm Comparison

    ### K-means Clustering
    - **Best for**: Spherical clusters, known number of clusters
    - **Advantages**: Fast, deterministic results, well-defined centroids
    - **Limitations**: Requires predefined K, assumes spherical clusters
    - **Use case**: When you have domain knowledge about expected cluster count

    ### DBSCAN Clustering
    - **Best for**: Arbitrary cluster shapes, unknown cluster count, noise detection
    - **Advantages**: Automatic cluster detection, handles outliers, flexible shapes
    - **Limitations**: Sensitive to parameters, may struggle with varying densities
    - **Use case**: Exploratory analysis, datasets with outliers

    ## Error Handling and Validation

    The class implements comprehensive error handling:
    - **Input Validation**: Checks file existence, format, and required columns
    - **Data Quality Control**: Removes invalid entries and reports data issues
    - **Algorithm Validation**: Ensures proper data preparation before clustering
    - **Output Validation**: Verifies successful file generation and directory creation

    ## Dependencies and Requirements

    - **pandas**: Data manipulation and CSV processing
    - **numpy**: Numerical computations and array operations
    - **matplotlib**: Visualization and plotting
    - **scikit-learn**: Machine learning algorithms (KMeans, DBSCAN, StandardScaler)
    - **pathlib**: Modern file path handling
    - **logging**: Comprehensive logging and debugging

    ## Performance Considerations

    - **Memory Usage**: Scales with dataset size; large datasets may require chunking
    - **Processing Speed**: K-means is generally faster than DBSCAN for large datasets
    - **Scalability**: Standardization improves clustering quality but adds processing time
    - **Visualization**: High-resolution plots may require significant memory for large datasets

    ## Common Use Cases

    1. **Object Detection Analysis**: Understanding bounding box size distributions in training data
    2. **Dataset Quality Assessment**: Identifying outliers and inconsistencies in annotations
    3. **Anchor Box Generation**: Creating appropriate anchor boxes for object detection models
    4. **Dataset Balancing**: Understanding class imbalances based on object sizes
    5. **Computer Vision Research**: Analyzing patterns in annotated datasets

    This class provides a robust, production-ready solution for bounding box clustering analysis
    with comprehensive documentation, error handling, and flexible configuration options.
    """

    def __init__(self, 
                 cluster_number: int,
                 input_file_path: Union[str, Path],
                 output_path: Union[str, Path],
                 output_filename: str):
        """
        Initialize the BoundingBoxClusteringProcessor.
        
        Args:
            cluster_number (int): Number of clusters to use for clustering
            input_file_path (Union[str, Path]): Path to the input CSV file
            output_path (Union[str, Path]): Directory where output files will be saved
            output_filename (str): Name of the output file (without extension)
        
        Raises:
            FileNotFoundError: If input CSV file doesn't exist
            ValueError: If cluster_number is not positive
        """
        self.cluster_number = cluster_number
        self.input_file_path = Path(input_file_path)
        self.output_path = Path(output_path)
        self.output_filename = output_filename
        
        # Set up logging
        self.logger = logging.getLogger(__name__)
        
        # Initialize data containers
        self.data = None
        self.clustering_data = None
        self.cluster_model = None
        self.cluster_labels = None
        self.scaler = None
        
        # Validate inputs
        self._validate_inputs()
        
        # Create an output directory
        self.output_path.mkdir(parents=True, exist_ok=True)
    
    def _validate_inputs(self) -> None:
        """Validate initialization parameters."""
        if not self.input_file_path.exists():
            raise FileNotFoundError(f"Input CSV file not found: {self.input_file_path}")
        
        if not self.input_file_path.suffix.lower() == '.csv':
            raise ValueError("Input file must be a CSV file")
        
        if self.cluster_number <= 0:
            raise ValueError("cluster_number must be positive")
    
    def load_csv_data(self) -> pd.DataFrame:
        """
        Load and validate CSV data from the input file.
        
        Returns:
            pd.DataFrame: Loaded data with required columns
            
        Raises:
            ValueError: If required columns are missing or data is invalid
        """
        try:
            self.data = pd.read_csv(self.input_file_path)
            self.logger.info(f"Loaded {len(self.data)} records from {self.input_file_path}")
            
            # Validate required columns
            required_columns = ['file_name', 'id', 'box_width', 'box_height']
            missing_columns = [col for col in required_columns if col not in self.data.columns]
            
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            # Check for valid numeric data
            numeric_columns = ['box_width', 'box_height']
            for col in numeric_columns:
                if not pd.api.types.is_numeric_dtype(self.data[col]):
                    raise ValueError(f"Column '{col}' must contain numeric data")
            
            # Remove rows with missing or invalid values
            initial_count = len(self.data)
            self.data = self.data.dropna(subset=numeric_columns)
            self.data = self.data[(self.data['box_width'] > 0) & (self.data['box_height'] > 0)]
            
            final_count = len(self.data)
            if final_count < initial_count:
                self.logger.warning(f"Removed {initial_count - final_count} invalid records")
            
            if final_count == 0:
                raise ValueError("No valid data remaining after filtering")
            
            return self.data
            
        except Exception as e:
            self.logger.error(f"Error loading CSV data: {e}")
            raise
    
    def redefine_dimensions(self, inplace: bool = True) -> pd.DataFrame:
        """
        Redefine box dimensions so that the largest value becomes box_width
        and the smaller value becomes box_height.
        
        Args:
            inplace (bool): Whether to modify the data in place
            
        Returns:
            pd.DataFrame: Data with redefined dimensions
        """
        if self.data is None:
            raise ValueError("No data loaded. Call load_csv_data() first.")
        
        data_to_process = self.data if inplace else self.data.copy()
        
        # Create masks for swapping
        swap_mask = data_to_process['box_height'] > data_to_process['box_width']
        
        if swap_mask.any():
            # Swap dimensions where height > width
            temp_width = data_to_process.loc[swap_mask, 'box_width'].copy()
            data_to_process.loc[swap_mask, 'box_width'] = data_to_process.loc[swap_mask, 'box_height']
            data_to_process.loc[swap_mask, 'box_height'] = temp_width
            
            swapped_count = swap_mask.sum()
            self.logger.info(f"Redefined dimensions for {swapped_count} records")
        
        if inplace:
            self.data = data_to_process
        
        return data_to_process
    
    def prepare_clustering_data(self) -> np.ndarray:
        """
        Prepare data for clustering by extracting box_width and box_height.
        
        Returns:
            np.ndarray: 2D array with shape (n_samples, 2) containing [box_width, box_height]
        """
        if self.data is None:
            raise ValueError("No data loaded. Call load_csv_data() first.")
        
        self.clustering_data = self.data[['box_width', 'box_height']].values
        self.logger.info(f"Prepared clustering data with shape: {self.clustering_data.shape}")
        
        return self.clustering_data

    def create_raw_data_scatter_plot(self,
                                     save_plot: bool = True,
                                     plot_filename: Optional[str] = None,
                                     figure_size: Tuple[int, int] = (12, 8),
                                     dpi: int = 200,
                                     color: str = 'steelblue',
                                     alpha: float = 0.6,
                                     marker_size: int = 50) -> None:
        """
        Create a 2D scatter plot of the raw data before clustering.

        This method generates a scatter plot showing all bounding box data points
        in a single color, providing a visual overview of the data distribution
        before any clustering analysis is applied.

        Args:
            save_plot (bool): Whether to save the plot to file
            plot_filename (Optional[str]): Custom filename for the plot (without extension)
            figure_size (Tuple[int, int]): Figure size in inches (width, height)
            dpi (int): DPI resolution for saved plot
            color (str): Color for all data points (matplotlib color name or hex code)
            alpha (float): Transparency level (0.0 to 1.0)
            marker_size (int): Size of scatter plot markers

        Raises:
            ValueError: If no clustering data is available
        """
        if self.clustering_data is None:
            raise ValueError(
                "No clustering data prepared. Call prepare_clustering_data() first.")

        plt.figure(figsize=figure_size)

        # Create scatter plot with all points in the same color
        plt.scatter(self.clustering_data[:, 0], self.clustering_data[:, 1],
                    c=color, s=marker_size, alpha=alpha, edgecolors='none')

        # Calculate basic statistics for the title
        n_points = len(self.clustering_data)
        width_range = (self.clustering_data[:, 0].min(),
                       self.clustering_data[:, 0].max())
        height_range = (self.clustering_data[:, 1].min(),
                        self.clustering_data[:, 1].max())

        # Set labels and title
        plt.xlabel('Box Width (pixels)')
        plt.ylabel('Box Height (pixels)')
        plt.title(f'Bounding Box Data Distribution (Before Clustering)\n'
                  f'Total data points: {n_points}\n'
                  f'Width range: {width_range[0]:.0f} - {width_range[1]:.0f} pixels, '
                  f'Height range: {height_range[0]:.0f} - {height_range[1]:.0f} pixels')

        # Add grid for better readability
        plt.grid(True, alpha=0.3)

        # Adjust layout
        plt.tight_layout()

        # Save plot if requested
        if save_plot:
            plot_name = plot_filename or f"{self.output_filename}_scatter_raw_data.png"
            plot_path = self.output_path / plot_name
            plt.savefig(plot_path, dpi=dpi, bbox_inches='tight')
            self.logger.info(f"Raw data scatter plot saved to: {plot_path}")

        # plt.show()
        plt.close()

    def apply_kmeans_clustering(self, random_state: int = 42) -> Tuple[np.ndarray, KMeans]:
        """
        Apply K-means clustering to the bounding box data.
        
        Args:
            random_state (int): Random state for reproducibility
            
        Returns:
            Tuple[np.ndarray, KMeans]: Cluster labels and fitted KMeans model
        """
        if self.clustering_data is None:
            raise ValueError("No clustering data prepared. Call prepare_clustering_data() first.")
        
        # Standardize the data for better clustering
        self.scaler = StandardScaler()
        scaled_data = self.scaler.fit_transform(self.clustering_data)
        
        # Apply K-means clustering
        self.cluster_model = KMeans(n_clusters=self.cluster_number, 
                                   random_state=random_state,
                                   n_init=10)
        self.cluster_labels = self.cluster_model.fit_predict(scaled_data)
        
        self.logger.info(f"Applied K-means clustering with {self.cluster_number} clusters")
        
        return self.cluster_labels, self.cluster_model
    
    def apply_dbscan_clustering(self, eps: float = 0.5, min_samples: int = 5) -> Tuple[np.ndarray, DBSCAN]:
        """
        Apply DBSCAN clustering to the bounding box data.
        
        Args:
            eps (float): Maximum distance between samples for them to be considered neighbors
            min_samples (int): Minimum number of samples in a neighborhood for a core point
            
        Returns:
            Tuple[np.ndarray, DBSCAN]: Cluster labels and fitted DBSCAN model
        """
        if self.clustering_data is None:
            raise ValueError("No clustering data prepared. Call prepare_clustering_data() first.")
        
        # Standardize the data for better clustering
        self.scaler = StandardScaler()
        scaled_data = self.scaler.fit_transform(self.clustering_data)
        
        # Apply DBSCAN clustering
        self.cluster_model = DBSCAN(eps=eps, min_samples=min_samples)
        self.cluster_labels = self.cluster_model.fit_predict(scaled_data)
        
        n_clusters = len(set(self.cluster_labels)) - (1 if -1 in self.cluster_labels else 0)
        n_noise = list(self.cluster_labels).count(-1)
        
        self.logger.info(f"Applied DBSCAN clustering: {n_clusters} clusters, {n_noise} noise points")
        
        return self.cluster_labels, self.cluster_model
    
    def create_scatter_plot(self, 
                           save_plot: bool = True,
                           plot_filename: Optional[str] = None,
                           figure_size: Tuple[int, int] = (12, 8),
                           dpi: int = 300) -> None:
        """
        Create a 2D scatter plot of the clustering results.
        
        Args:
            save_plot (bool): Whether to save the plot
            plot_filename (Optional[str]): Custom filename for the plot
            figure_size (Tuple[int, int]): Figure size in inches
            dpi (int): DPI for saved plot
        """
        if self.clustering_data is None or self.cluster_labels is None:
            raise ValueError("No clustering results available. Apply clustering first.")
        
        plt.figure(figsize=figure_size)
        
        # Get unique cluster labels
        unique_labels = np.unique(self.cluster_labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
        
        # Plot each cluster with different colors
        for label, color in zip(unique_labels, colors):
            if label == -1:  # Noise points for DBSCAN
                cluster_data = self.clustering_data[self.cluster_labels == label]
                plt.scatter(cluster_data[:, 0], cluster_data[:, 1], 
                           c='black', marker='x', s=50, alpha=0.6, label='Noise')
            else:
                cluster_data = self.clustering_data[self.cluster_labels == label]
                plt.scatter(cluster_data[:, 0], cluster_data[:, 1], 
                           c=[color], s=50, alpha=0.7, label=f'Cluster {label}')
        
        # Add cluster centers for K-means
        if isinstance(self.cluster_model, KMeans):
            centers = self.scaler.inverse_transform(self.cluster_model.cluster_centers_)
            plt.scatter(centers[:, 0], centers[:, 1], 
                       c='red', marker='X', s=200, linewidths=2, 
                       label='Centroids', edgecolors='black')
        
        plt.xlabel('Box Width (pixels)')
        plt.ylabel('Box Height (pixels)')
        plt.title(f'Bounding Box Clustering Results\n'
                 f'Algorithm: {type(self.cluster_model).__name__}, '
                 f'Data points: {len(self.clustering_data)}')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_plot:
            plot_name = plot_filename or f"{self.output_filename}_scatter_plot.png"
            plot_path = self.output_path / plot_name
            plt.savefig(plot_path, dpi=dpi, bbox_inches='tight')
            self.logger.info(f"Scatter plot saved to: {plot_path}")
        
        # plt.show()
        plt.close()
    
    def save_cluster_centers(self, centers_filename: Optional[str] = None) -> None:
        """
        Save cluster centers to a text file.
        
        Args:
            centers_filename (Optional[str]): Custom filename for the centers file
        """
        if self.cluster_model is None:
            raise ValueError("No clustering model available. Apply clustering first.")
        
        centers_name = centers_filename or f"{self.output_filename}_cluster_centers.txt"
        centers_path = self.output_path / centers_name
        
        with open(centers_path, 'w') as f:
            f.write(f"Cluster Centers Analysis\n")
            f.write(f"Algorithm: {type(self.cluster_model).__name__}\n")
            f.write(f"Input file: {self.input_file_path}\n")
            f.write(f"Total data points: {len(self.clustering_data)}\n")
            f.write(f"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*50 + "\n\n")
            
            if isinstance(self.cluster_model, KMeans):
                # For K-means, we have explicit cluster centers
                centers = self.scaler.inverse_transform(self.cluster_model.cluster_centers_)
                f.write(f"K-means Cluster Centers:\n")
                f.write(f"Number of clusters: {len(centers)}\n\n")
                
                for i, center in enumerate(centers):
                    f.write(f"Cluster {i}:\n")
                    f.write(f"  Box Width:  {center[0]:.2f} pixels\n")
                    f.write(f"  Box Height: {center[1]:.2f} pixels\n")
                    
                    # Count points in this cluster
                    cluster_size = np.sum(self.cluster_labels == i)
                    f.write(f"  Data points: {cluster_size}\n\n")
                    
            elif isinstance(self.cluster_model, DBSCAN):
                # For DBSCAN, calculate cluster centers manually
                unique_labels = np.unique(self.cluster_labels)
                f.write(f"DBSCAN Cluster Centers (calculated):\n")
                f.write(f"Number of clusters: {len(unique_labels[unique_labels != -1])}\n")
                if -1 in unique_labels:
                    noise_points = np.sum(self.cluster_labels == -1)
                    f.write(f"Noise points: {noise_points}\n")
                f.write("\n")
                
                for label in unique_labels:
                    if label == -1:
                        continue  # Skip noise points
                    
                    cluster_data = self.clustering_data[self.cluster_labels == label]
                    center = np.mean(cluster_data, axis=0)
                    
                    f.write(f"Cluster {label}:\n")
                    f.write(f"  Box Width:  {center[0]:.2f} pixels\n")
                    f.write(f"  Box Height: {center[1]:.2f} pixels\n")
                    f.write(f"  Data points: {len(cluster_data)}\n\n")
        
        self.logger.info(f"Cluster centers saved to: {centers_path}")
    
    def get_clustering_summary(self) -> Dict[str, Any]:
        """
        Get a summary of the clustering results.
        
        Returns:
            Dict[str, Any]: Summary statistics and information
        """
        if self.cluster_labels is None:
            raise ValueError("No clustering results available. Apply clustering first.")
        
        unique_labels = np.unique(self.cluster_labels)
        n_clusters = len(unique_labels[unique_labels != -1])
        n_noise = np.sum(self.cluster_labels == -1) if -1 in unique_labels else 0
        
        summary = {
            'algorithm': type(self.cluster_model).__name__,
            'total_data_points': len(self.clustering_data),
            'n_clusters': n_clusters,
            'n_noise_points': n_noise,
            'cluster_sizes': {},
            'data_range': {
                'box_width': (float(self.clustering_data[:, 0].min()), 
                             float(self.clustering_data[:, 0].max())),
                'box_height': (float(self.clustering_data[:, 1].min()), 
                              float(self.clustering_data[:, 1].max()))
            }
        }
        
        # Calculate cluster sizes
        for label in unique_labels:
            if label == -1:
                summary['cluster_sizes']['noise'] = int(n_noise)
            else:
                summary['cluster_sizes'][f'cluster_{label}'] = int(np.sum(self.cluster_labels == label))
        
        return summary

    def process_complete_workflow(self,
                                  algorithm: str = 'kmeans',
                                  redefine_dims: bool = False,
                                  create_raw_plot: bool = True,
                                  **algorithm_params) -> Dict[str, Any]:
        """
        Execute the complete clustering workflow.

        Args:
            algorithm (str): Clustering algorithm ('kmeans' or 'dbscan')
            redefine_dims (bool): Whether to redefine dimensions
            create_raw_plot (bool): Whether to create raw data scatter plot before clustering
            **algorithm_params: Additional parameters for the clustering algorithm

        Returns:
            Dict[str, Any]: Summary of the clustering results
        """
        try:
            # Load data
            self.load_csv_data()

            # Redefine dimensions if requested
            if redefine_dims:
                self.redefine_dimensions()

            # Prepare clustering data
            self.prepare_clustering_data()

            # Create raw data plot if requested
            if create_raw_plot:
                self.create_raw_data_scatter_plot()

            # Apply clustering
            if algorithm.lower() == 'kmeans':
                self.apply_kmeans_clustering(**algorithm_params)
            elif algorithm.lower() == 'dbscan':
                self.apply_dbscan_clustering(**algorithm_params)
            else:
                raise ValueError(f"Unsupported algorithm: {algorithm}")

            # Create visualizations and save results
            self.create_scatter_plot()
            self.save_cluster_centers()

            # Return summary
            summary = self.get_clustering_summary()
            self.logger.info("Complete workflow executed successfully")

            return summary

        except Exception as e:
            self.logger.error(f"Error in complete workflow: {e}")
            raise

# # Example usage
# if __name__ == "__main__":
#     # Set up logging
#     logging.basicConfig(
#         level=logging.INFO,
#         format='%(asctime)s - %(levelname)s - %(message)s'
#     )
#
#     # Example with K-means
#     processor = BoundingBoxClusteringProcessor(
#         cluster_number=5,
#         input_file_path="path/to/bounding_boxes.csv",
#         output_path="path/to/output",
#         output_filename="clustering_results"
#     )
#
#     # Execute the complete workflow
#     summary = processor.process_complete_workflow(
#         algorithm='kmeans',
#         redefine_dims=True,
#         random_state=42
#     )
#
#     print("Clustering Summary:")
#     for key, value in summary.items():
#         print(f"  {key}: {value}")
#
#     # Example with DBSCAN
#     processor_dbscan = BoundingBoxClusteringProcessor(
#         cluster_number=5,  # Not used for DBSCAN but required for initialization
#         input_file_path="path/to/bounding_boxes.csv",
#         output_path="path/to/output",
#         output_filename="clustering_results_dbscan"
#     )
#
#     summary_dbscan = processor_dbscan.process_complete_workflow(
#         algorithm='dbscan',
#         redefine_dims=True,
#         eps=0.5,
#         min_samples=5
#     )
#
#
# ## **Usage Examples:**
# ### **Standalone Usage**

# # After loading and preparing data
# processor.load_csv_data()
# processor.prepare_clustering_data()

# # Create raw data plot with default settings
# processor.create_raw_data_scatter_plot()

# # Or with custom settings
# processor.create_raw_data_scatter_plot(
#     color='darkgreen',
#     alpha=0.7,
#     marker_size=30,
#     figure_size=(10, 6)
# )


# ### **As Part of Complete Workflow:**

# # Include raw data plot in complete workflow
# summary = processor.process_complete_workflow(
#     algorithm='kmeans',
#     redefine_dims=True,
#     create_raw_plot=True,  # This will generate the raw data plot
#     random_state=42
# )

# # Or disable raw data plot
# summary = processor.process_complete_workflow(
#     algorithm='kmeans',
#     create_raw_plot=False,  # Skip raw data plot
#     random_state=42
# )

